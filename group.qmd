---
title: "Group Task"
author:
  - "Marco Boso 100535153"
  - "Diego Paroli 100554973"
  - "Yijia Lin 100452242"
  - "Bradley McKenzie 100535241"
  - "Linghan Zheng 100540803"
  - "Jia Lin 100536210"
  - "Isabel Monge 100542532"
format:
  html:
    theme: [style.scss]
    toc: true
    toc-location: right
    toc-title: Índice
    embed-resources: true
---

## Objectives and mandatory items

The objective of the delivery is to perform an analysis of the electoral data, carrying out the debugging, summaries and graphs you consider, both of their results and the accuracy of the electoral polls.

Specifically, **you must work only in the time window that includes the elections from 2008 to the last elections of 2019**.

### General comments

In addition to what you see fit to execute, the following items are mandatory:

-   Each group should **present before 9th January (23:59) an analysis of the data** in `.qmd` and `.html` format in **Quarto slides** mode, which **will be the ones they will present on the day of the presentation**.

-   **Quarto slides should be uploaded to Github** (the link should be provided by a member of each group).

-   The **maximum number of slides** should be 40. The **maximum time for each group** will be 20-22 minutes (+5 minutes for questions).

-   During the presentation you will **explain (summarised!) the analysis performed** so that **each team member speaks for a similar amount of time** and **each member can be asked about any of the steps**. The grade does not have to be the same for all members.

-   It will be valued not only the content but also the container (aesthetics).

-   The **objective is to demonstrate that the maximum knowledge of the course has been acquired**: the more content of the syllabus is included, the better.

### Mandatory items:

1.  Data should be **converted to tidydata** where appropriate.

2.  You should **include at least one join** between tables.

3.  Reminder: information = variance, so **remove columns that are not going to contribute anything**.

4.  The **glue and lubridate** packages should be used at some point, as well as the **forcats**. The use of **ggplot2** will be highly valued.

5.  The following should be used at least once:

    -   mutate
    -   summarise
    -   group_by (or equivalent)
    -   case_when

6.  We have many, many parties running for election. **We will only be interested in the following parties**:

    -   PARTIDO SOCIALISTA OBRERO ESPAÑOL (beware: it has/had federations - branches - with some other name).
    -   PARTIDO POPULAR
    -   CIUDADANOS (caution: has/had federations - branches - with some other name)
    -   PARTIDO NACIONALISTA VASCO
    -   BLOQUE NACIONALISTA GALLEGO
    -   CONVERGÈNCIA I UNIÓ
    -   UNIDAS PODEMOS - IU (beware that here they have had various names - IU, podem, ezker batua, ...- and have not always gone together, but here we will analyze them together)
    -   ESQUERRA REPUBLICANA DE CATALUNYA
    -   EH - BILDU (are now a coalition of parties formed by Sortu, Eusko Alkartasuna, Aralar, Alternatiba)
    -   MÁS PAÍS
    -   VOX

7.  Anything other than any of the above parties should be imputed as "OTHER". Remember to add properly the data after the previous recoding.

8.  Party acronyms will be used for the visualizations. The inclusion of graphics will be highly valued (see <https://r-graph-gallery.com/>).

9.  You must use all 4 data files at some point.

10. You must define at least one (non-trivial) function of your own.

11. You will have to discard mandatory polls that:

```         
-   refer to elections before 2008
-   that are exit polls
-   have a sample size of less than 750 or are unknown
-   that have less than 1 or less fieldwork days
```

12. You must obligatorily answer the following questions (plus those that you consider analyzing to distinguish yourself from the rest of the teams, either numerically and/or graphically)

```         
-   Which party was the winner in the municipalities with more than 100,000 habitants (census) in each of the elections?
-   Which party was the second when the first was the PSOE? And when the first was the PP?
-   Who benefits from low turnout?
-   How to analyze the relationship between census and vote? Is it true that certain parties win in rural areas?
-   How to calibrate the error of the polls (remember that the polls are voting intentions at national level)?
-   Which polling houses got it right the most and which ones deviated the most from the results?
```

**You should include at least 3 more "original" questions** that you think that it could be interesting to be answer with the data.

### Marks

**The one who does the most things will not be valued the most**. More is not always better. The **originality** (with respect to the rest of the works, for example in the analyzed or in the subject or ...) of what has been proposed, in the handling of tables (or in the visualization), the **caring** put in the delivery (care in life is important) and the **relevance** of what has been done will be valued. Once you have the mandatory items with your database more or less completed, **think before** chopping code: what could be interesting? What do I need to get a summary both numerical and visual?

Remember that the real goal is to demonstrate a mastery of the tools seen throughout the course. And that happens not only by the quantity of them used but also by the quality when executing them.

**Some dataviz will be extremely positive valued**.

## Required packages

> Insert in the lower chunk the packages you will need

```{r}
rm(list = ls())
library(tidyverse)
library(dplyr)
library(tidyr)
library(stringr)
library(lubridate)
library(DataExplorer)
library(glue)
library(ggplot2)
library(forcats)
library(ggrepel)
library(scales)
library(ggpubr)
```

## Data

The practice will be based on the **electoral data archives** below, compiling data on elections to the Spanish Congress of Deputies from 2008 to the present, as well as surveys, municipalities codes and abbreviations.

```{r}
# NO TOQUES NADA
election_data <- read_csv(file = "./data/datos_elecciones_brutos.csv")
cod_mun <- read_csv(file = "./data/cod_mun.csv")
surveys <- read_csv(file = "./data/historical_surveys.csv")
abbrev <- read_csv(file = "./data/siglas.csv")
```

The data will be as follows:

-   `election_data`: file with election data for Congress from 2018 to the last ones in 2019.

    -   `tipo_eleccion`: type of election (02 if congressional election)
    -   `anno`, `mes`: year and month of elections
    -   `vuelta`: electoral round (1 if first round)
    -   `codigo_ccaa, codigo_provincia, codigo_municipio, codigo_distrito_electoral`: code of the ccaa, province, municipality and electoral district.
    -   `numero_mesas`: number of polling stations
    -   `censo`: census
    -   `participacion_1, participacion_2`: participation in the first preview (14:00) and second preview (18:00) before polls close (20:00)
    -   `votos_blancos`: blank ballots
    -   `votos_candidaturas`: party ballots
    -   `votos_nulos`: null ballots
    -   ballots for each party

-   `cod_mun`: file with the codes and names of each municipality

-   `abbrev`: acronyms and names associated with each party

-   `surveys`: table of electoral polls since 1982. Some of the variables are the following:

    -   `type_survey`: type of survey (national, regional, etc.)
    -   `date_elec`: date of future elections
    -   `id_pollster`, `pollster`, `media`: id and name of the polling company, as well as the media that commissioned it.
    -   `field_date_from`, `field_date_to`: start and end date of fieldwork
    -   `exit_poll`: whether it is an exit poll or not
    -   `size`: sample size
    -   `turnout`: turnout estimate
    -   estimated voting intentions for the main parties

### Cleaning the data -- surveys

```{r cleaning survey df}

# Filter dataset
cleaned_surveys <- surveys |>
  mutate(
    # Parse dates variables as date objects
    field_date_from = ymd(field_date_from),
    field_date_to = ymd(field_date_to),
    date_elec = ymd(date_elec),
    # Calculate the number of fieldwork days
    fieldwork_days = as.numeric(field_date_to - field_date_from + 1)
  ) |>
  filter(
    !exit_poll,                           # Exclude exit polls
    date_elec >= ymd("2008-01-01"),       # Exclude polls referred to elections before 2008
    size >= 750,                          # Exclude polls with sample size < 750
    fieldwork_days > 1                    # Exclude polls with 1 or fewer fieldwork days
  )

# Deleting columns that only have NAs
cleaned_surveys <- cleaned_surveys |> 
  select(where(~ !all(is.na(.))))

# Identify party columns dynamically
metadata_columns <- c("type_survey", "date_elec", "id_pollster", "pollster", "media",
                      "field_date_from", "field_date_to", "fieldwork_days", "exit_poll", 
                      "size", "turnout")
party_columns <- setdiff(colnames(cleaned_surveys), metadata_columns)

# Reshape data into long format
tidy_surveys <- cleaned_surveys |>
  pivot_longer(
    cols = all_of(party_columns),  # Reshape party columns
    names_to = "party_raw",        # Raw party names
    values_to = "votes"            # Corresponding voting intentions
  )

# add on party names by code
tidy_surveys <- tidy_surveys %>%
  mutate(
    party = case_when(
      party_raw == "PSOE" ~ "PARTIDO SOCIALISTA OBRERO ESPAÑOL",
      party_raw == "CIU" ~ "CONVERGÈNCIA I UNIÓ",
      party_raw == "EAJ-PNV" ~ "PARTIDO NACIONALISTA VASCO",
      party_raw == "ERC" ~ "ESQUERRA REPUBLICANA DE CATALUNYA",
      party_raw == "IU" ~ "UNIDAS PODEMOS - IU",
      party_raw == "PP" ~ "PARTIDO POPULAR",
      party_raw == "BNG" ~ "BLOQUE NACIONALISTA GALLEGO",
      party_raw == "CS" ~ "CIUDADANOS",
      party_raw == "EH-BILDU" ~ "EH - BILDU",
      party_raw == "PODEMOS" ~ "UNIDAS PODEMOS - IU",
      party_raw == "VOX" ~ "VOX",
      party_raw == "MP" ~ "MÁS PAÍS",
      TRUE ~ "OTHER")
  )

# Create a column for proper, unqique acronyms
tidy_surveys <- tidy_surveys |> 
  mutate(
    party_code = case_when(
      party == "UNIDAS PODEMOS - IU"~ "PODEMOS-IU",
      party == "OTHER"~ "OTHER",
      TRUE ~ party_raw)
  )

# Select relevant columns
# Getting rid of type_survey, exit_poll (take only 1 value), party_raw
final_surveys <- tidy_surveys |>
  select(-type_survey, -exit_poll, -party_raw) |> 
  relocate(fieldwork_days, .after = field_date_to) |> 
  relocate(votes, .after = party_code) 

# Summing all votes based on the party reclassification
final_surveys <- final_surveys |> 
  group_by(across(-votes)) |> 
  summarize(votes = sum(votes, na.rm = TRUE), .groups = "drop") |> 
  arrange(field_date_from)
# We have 1614 surveys (rows from cleaned_surveys), 12 parties (meaning 12 rows per survey). Thus 1614x12=19368 rows

# Preview
final_surveys
```

### Creating table for party codes

Creating a table to link each party name to its unique code

```{r}
party_info <- final_surveys |> 
  select(party, party_code) |> 
  unique()
```

### Cleaning the data -- election_data

The election_data file is large and requires quite extensive cleaning to make it "tidy". We will tidy the data to try make it most useful for future analysis. The election data starts off with 48,737 rows and 471 columns. Reducing the number of columns is a clear priority.

First, we look at the quality of the data and see if any information is redundant and can be removed.

```{r check-electiondata-quality}
plot_intro(election_data)

# We see 1.9% missing colums, identify the cols with no data - we have 9 cols. 
blank_cols <- names(election_data)[sapply(election_data, function(x) all(is.na(x)))]

# Drop these columns and also filter to ensure no info outside 2008 to 2019 is included. 
election_data <- election_data |> 
  select(-all_of(blank_cols)) |> 
  filter(anno >= 2008 & anno <= 2019)

# Drop columns that are logical
election_data <- election_data %>%
  select(where(~ !is.logical(.)))
```

```{r}
# See the improvements
plot_intro(election_data)
```

Second, we begin to make the election data tidy. We start by pivoting the data so all columns for party names are within one "party" variable. Before this we have 414 columns referring to parties.

```{r pivot-election}
# Pivot all the party names and ballot counts to the main table
election_pivot <- election_data |> 
  pivot_longer(
    cols = `BERDEAK-LOS VERDES`:`COALICIÓN POR MELILLA`, # select all party data
    names_to = "party",
    values_to = "ballots"
  )
str(election_pivot)
head(election_pivot)
```

We now have a table with 20,177,118 rows and 17 columns.

This is more clean than previously, but we still need to aggregate of our party variables into the main party groups. We will do this by creating a mapping table (party_names) that standardizes the raw party names into main party groupings (party_main) using regular expressions.

```{r assigning-partygroup}
party_names <- tibble(names = unique(election_pivot$party))

# Party names in the election_data file do not match up perfectly with the abbrev file (i.e. some of the names present in party_names are not in abbrev)
# So it is better to work directly on party_names instead of using abbrev

party_names <- party_names |> 
    mutate(party_main = case_when(
                str_detect(names, "(?i)PSOE|PARTIDO DOS SOCIALISTAS DE GALICIA|PARTIDO SOCIALISTA DE EUSKADI|PARTIDO SOCIALISTA OBRERO ESPAÑOL|PARTIT SOCIALISTA OBRER ESPANYOL") ~ "PARTIDO SOCIALISTA OBRERO ESPAÑOL",
                str_detect(names, "(?i)PARTIDO POPULAR") ~ "PARTIDO POPULAR",
                str_detect(names, "(?i)CIUDADANOS-PARTIDO DE LA CIUDADANIA|CIUDADANOS-PARTIDO DE LA CIUDADANÍA|CIUDADANOS PARTIDO DE LA CIUDADANIA|CIUDADANOS PARTIDO DE LA CIUDADANÍA|CIUDADANOS, PARTIDO DE LA CIUDADANÍA|CIUTADANS") ~ "CIUDADANOS",
                str_detect(names, "(?i)EUZKO ALDERDI JELTZALEA-PARTIDO NACIONALISTA VASCO") ~ "PARTIDO NACIONALISTA VASCO",
                str_detect(names, "(?i)BLOQUE NACIONALISTA GALEGO|BNG") ~ "BLOQUE NACIONALISTA GALLEGO",
                str_detect(names, "(?i)CONVERGENCIA I UNIO|CONVERGÈNCIA I UNIÓ") ~ "CONVERGÈNCIA I UNIÓ",
                str_detect(names, "(?i)PODEM|EZKER BATUA|EZKER ANITZA|IZQUIERDA UNIDA|ESQUERRA UNIDA|ESQUERDA UNIDA") ~ "UNIDAS PODEMOS - IU",
                str_detect(names, "(?i)ESQUERRA REPUBLICANA") ~ "ESQUERRA REPUBLICANA DE CATALUNYA",
                str_detect(names, "(?i)BILDU|EUSKO ALKARTASUNA|ARALAR|SORTU|ALTERNATIBA") ~ "EH - BILDU",
                str_detect(names, "(?i)MÁS PAÍS") ~ "MÁS PAÍS",
                str_detect(names, "(?i)VOX") ~ "VOX",
                TRUE ~ "OTHER")
    )

unique(party_names$party_main)

# Adding party code to party_names dataframe
party_names <- party_names |> 
  left_join(party_info, by = join_by(party_main == party))
```

Now join on the main party names and codes to our election table. Testing was undertaken and the join of a table was more efficient than alternatives (e.g. str_detects over election_pivot or rowwise summaries).

```{r join-partygroup}
# Join party main and party code into main df
election_pivot <- election_pivot |> 
  left_join(party_names, by = join_by(party == names))
```

Now we will include some additional information that will make the analysis potentially easier later, including province and total votes counts from our data:

```{r join-municipality-names}
# Create municipal code to join on municipal names. 
# Create total votes column
election_pivot <- election_pivot |>
  mutate(cod_mun = paste(codigo_ccaa, codigo_provincia, codigo_municipio, sep="-"),
         total_votes = votos_blancos + votos_nulos + votos_candidaturas)

# Join municipality names
election_pivot <- election_pivot |> 
  left_join(cod_mun, by = join_by(cod_mun))  

# Check quality of the join and whether NA's have been introduced as municipality names
any(is.na(election_pivot$municipio))
```

Be careful not all 8135 municipalities appear in each election. We have 6 elections and 414 parties, thus we should have 6x414=2484 occurrences for each municipality, but that is not the case.

Also be careful some municipalities have the same name (but different mun_code), so if you ever need to group by municipality remember to group by mun_code instead of municipality.

```{r}
# Count the number of times each municipaly appears and then get the unique values for that count (not all are 2484) meaning some municipalities are not present in certain elections
election_pivot |> count(cod_mun) |> pull(n) |> unique()

#Number of unique values for cod_mun is different than number of unique values for municipio
n_distinct(cod_mun$cod_mun)
n_distinct(cod_mun$municipio)
```

Now we need to group together all of the votes for "OTHER" variables and create unique identifiers for each individual election in our dataframes.

Currently we have a table of 22 variables with 20,177,118 rows. We can clean this more.

First, identify the redundant data in our election. We can remove:

tipo_eleccion - because all values = 02. It is not useful vuelta = because all values = 1, it is not useful. geographic variables = we will remove `codigo_municipio` is included in `cod_mun` which we joined on from the *cod_mun* table. We keep the autonomous community and proivnce variables for potential future aggregation and analysis. codigo_distrito_electoral - because every value is zero. It is not useful.

Notably, we have many NA ballot rows and a row for each individual party at each election, where will also try to reduce this when we aggregate the party data with the "party_main" variable created.

```{r tidy-election-selecting-variables}
# To clean the data more, reduce our dataset and rename key variables so everything is more consistent in English
tidy_election <- election_pivot |> 
  select(year = anno, 
         month = mes,
         code_community = codigo_ccaa,
         code_province = codigo_provincia,
         code_municipality = cod_mun,
         municipality = municipio,
         population = censo,
         polling_stations = numero_mesas,
         participation_1 = participacion_1,
         participation_2 = participacion_2,
         blank_votes = votos_blancos,
         null_votes = votos_nulos,
         valid_votes = votos_candidaturas,
         total_votes,
         party_main,
         party_code,
         ballots)

summary(tidy_election)
```

```{r tidy-election-aggregating parties}
tidy_election <- tidy_election |> 
  group_by(across(-ballots))|> 
  summarise(party_ballots = sum(ballots, na.rm=TRUE), .groups = "drop")

tidy_election
```

Joining year and month into one variable

```{r creating-date-variable}
final_election <- tidy_election |> 
  mutate(date_elec = glue("{year}-{month}-01")) |> 
  relocate(date_elec, .before = year) |> 
  select(-year, -month)

#Adding correct days to match survey dataframe
final_election <- final_election |> 
  mutate(
    date_elec = ymd(case_when(
      date_elec == "2008-03-01" ~ "2008-03-09",
      date_elec == "2011-11-01" ~ "2011-11-20",
      date_elec == "2015-12-01" ~ "2015-12-20",
      date_elec == "2016-06-01" ~ "2016-06-26",
      date_elec == "2019-04-01" ~ "2019-04-28",
      date_elec == "2019-11-01" ~ "2019-11-10"))
  )

str(final_election)
```

Election identifiers:

-   Timing -\> date
-   Area information -\> code_community (autonomous community), code_province, code_municipality, municipality, population
-   General election information -\> polling_stations, participation_1, participation_2, blank_votes, null_votes, valid_votes, total_votes
-   Party votes received -\> party_main, party_code, party_ballots

### Creating turnout dataframe

Creating a dataframe storing all the turnout data for each municipality in each election in case we need to work just on turnout or other data that does not change by party.

All this info is still included in final_election

```{r}
turnout <- final_election |> 
  select(
    date_elec, code_community, code_province, code_municipality, municipality,
    population, polling_stations, participation_1, participation_2, 
    blank_votes, null_votes, valid_votes, total_votes
  ) |> 
  distinct()
turnout
```

### Recap cleaning

We have 2 primary datasets at this stage, election data and survey data, plus a turnout dataframe which is a subset of the election data. For surveys, the data has been cleaned so each row represents the votes for one party within a specific national poll. For elections, the data has been cleaned so each row represents the number of votes for a party within an election in a specific municipality.

The `final_surveys` data includes:

-   election date, pollster and media information, fieldwork dates
-   size of the survey and turnout
-   party name, party code
-   votes received (for that party in that poll)

The `final_election` data includes:

-   date of the election
-   party name, party code (with non-primary parties grouped)
-   identifier for autonomous community, province and municipality
-   municipality population
-   election information such as number of polling stations, votes per session
-   ballots received (for that party per election in each municipality)

The `turnout` data includes:

-   information on the number of votes and type of vote (e.g. valid or blank/null) per municipality in each election.

**!!!!!!! WORK ON DATAFRAMES final_surveys, final_election, turnout !!!!!!!**\
**!!!!!!! DO NOT OVERWRITE THESE DATAFRAMES, CREATE NEW ONES IF YOU NEED TO MODIFY THEM (ex. surveys_q1 \<- final_surveys) !!!!!!!**

## Mandatory questions

### 1.Which party was the winner in the municipalities with more than 100,000 habitants (census) in each of the elections?

```{r}

```

### 2. Which party was the second when the first was the PSOE? And when the first was the PP?

```{r}

```

### 3. Who benefits from low turnout?



### 4. How to analyze the relationship between census and vote? Is it true that certain parties win in rural areas?

```{r}

```

### 5. How to calibrate the error of the polls (remember that the polls are voting intentions at national level)?

```{r}

```

### 6. Which polling houses got it right the most and which ones deviated the most from the results?

Before starting with this question, it's interesting to discuss how to measure the goodness of prediction of one pollsters. 

In our view, this perspective is somewhat narrow if it is only based on accurately predicting the winning party (or the one with the most votes) in each case:  studies in this assignment have found that there have been a number of recent instances in Spain where large parties with more votes have not been able to form an independent government, and that large political parties have often needed to collaborate with smaller parties.

However, it is also inappropriate to consider predictions of results for all parties (hundreds) as equally important, as many small parties end up not even getting a single parliamentary seat.

Therefore, our measure in this question is to divide the parties into two parts:
Assign a weight of 0.7 to the top five parties receiving the most votes in each general election, and a weight of 0.3 to the remaining parties.

In this way, the top five parties with the most votes get more weight than the other small parties, but we do not exclude the small parties completely from our accuracy assessment.


Haven defined the criteria, firstly, we will summarise the prediction of each pollster for each party in each of these elections, as well as the party ballots rate in every election. Then, we join these two tables with the key `date_elec_party_code` for later calculation.

```{r}
pollster_estimate <- final_surveys |> 
  select(date_elec, pollster, party_code, votes) |> 
  mutate(date_elec_party_code = paste(date_elec, party_code, sep = "_")) |> 
  select(date_elec_party_code, pollster, votes)

final_election_summary <- final_election |> 
  group_by(date_elec, party_code) |> 
  reframe(sum_ballots = sum(party_ballots)) 

total_votes_summary<- final_election |> 
  dplyr::select(-party_main, -party_code, -party_ballots) |> 
  distinct() |> 
  group_by(date_elec) |> 
  summarise(sum_total_votes=sum(valid_votes))

final_election_summary<- final_election_summary |> 
  left_join(total_votes_summary, by=c("date_elec"="date_elec")) |> 
  mutate(national_share = sum_ballots/sum_total_votes) |> 
  mutate(date_elec_party_code = paste(date_elec, party_code, sep = "_")) |> 
  left_join(pollster_estimate, by=c("date_elec_party_code"="date_elec_party_code")) |> 
  dplyr::select(-date_elec_party_code) 
```

Now we will calculate the absolute error of pollsters predictions comparing with real result, assigning weight of 0.7 to the first 5 large parties in each election, and 0.3 to the remaining parties.

```{r}
final_election_summary_v2 <- final_election_summary |> 
  group_by(date_elec) |>  
  arrange(date_elec, desc(national_share)) |>  
  mutate(rank_pos = dense_rank(-national_share)) |>  
  mutate(weight = if_else(rank_pos <= 5, 0.7, 0.3)) |>  
  ungroup()
```

Then, we can now calculate the weighted MAE of each pollster:

```{r}
final_election_summary_v3 <- final_election_summary_v2 |> 
  mutate(national_share=national_share*100) |> 
  mutate(error = abs(votes - national_share)) 
  
wmae <- final_election_summary_v3 |> 
  group_by(pollster) |> 
  summarise(WMAE = sum(weight * error) / sum(weight)) |> 
  ungroup() |> 
  mutate(pollster = fct_reorder(pollster, WMAE, .desc = FALSE))
```

We can obtain the 5 pollsters that predict the best and the 5 that predict the worst.

```{r}
head(wmae$pollster[order(wmae$WMAE)], 5)
```

```{r}
tail(wmae$pollster[order(wmae$WMAE)], 5)
```

Here we will visualize the performance of these pollsters:

```{r WMAE,warning = FALSE}
ggplot(wmae, aes(x = pollster, y = WMAE, fill = WMAE)) +
  geom_col() +  
  theme_minimal(base_size = 10) +
  theme(
    panel.grid.major = element_line(color = "grey80", size = 0.5),  
    panel.grid.minor = element_blank(), 
    axis.text.x = element_text(angle = 65, hjust = 1, size = 8),
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(face = "bold", size = 12),
    axis.title.y = element_text(face = "bold", size = 10)
  ) +
  labs(
    title = "Weighted Average Polling Error by Pollster",
    subtitle = "Comparison of polling errors across different organizations",
    x = NULL,
    y = "Weighted Average Error"
  ) +
  scale_y_continuous(labels = scales::comma) +
  scale_fill_gradient2(low = "green", mid = "#ffc403", high = "#c70b1f", midpoint = 3.2) 

```


## Additional questions

Confirm with the group your original analysis question to avoid clashing ideas :)

SOME IDEAS FOR THE ORIGINAL QUESTIONS TO START?

-   Which regions had the most predictable votes (i.e. consistently voted for the same party) and which regions were the most undecided (i.e. had the most variance in there votes across) between the 2008 and 2019 elections?

-   Map the outcomes over time - plotly on the results? Isabelle has some interest but tbc if we will Map.

-   Can we load in other data? think that would go down well? Maybe predict the next election results based on previous trends of the 5 years and compare to see if the following election followed the trend? Think Javi would be happy with new data.

-   Which municipalities voting patterns were most consistent with the national trends?

-   Which 2 media/pollster outlets had the most polarized estimates of each election?

### 7. Jacklyn and Linghan

```{r}

```

### 8. Yijia and Diego

How has the turnout rate changed over time? And within each election year, how are turnout rates correlated with the municipalities' populations?

```{r}
question8<- final_election |> 
  group_by(date_elec,municipality) |> 
  summarize(population=mean(population),
            total_votes=mean(total_votes),
            turnout_rate=(total_votes/population),
            .groups = "drop") 
```

```{r,warning = FALSE}
turnout_over_time <- question8 |> 
  group_by(date_elec) |> 
  summarize(avg_turnout = (sum(total_votes) / sum(population)),
                           na.rm = TRUE, 
                           .groups = "drop")

ggplot(turnout_over_time, aes(x = date_elec, y = avg_turnout)) +
  geom_line(color = "#ffc403", size = 2) +  
  geom_point(color = "#c70b1f", size = 3) +  
  geom_text_repel(aes(label = scales::percent(avg_turnout, accuracy = 0.01)),
                  size = 3, 
                  box.padding = 0.5,
                  segment.color = NA,
                  color="#c70b1f") +
  labs(
    title = "Change in Turnout Rate Over Time",
    x = "Election Date",
    y = "Average Turnout Rate"
  ) +
  scale_y_continuous(
    limits = c(0.6, 0.8), 
    labels = percent_format(scale = 100) 
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12)
  )
```

We can observe that there is no clear tendency of turnout rate. However, it is interesting seeing that these two 'snap' elections in 2016 and 2019 were held shortly after the previous elections, with a very brief gap between each one and its predecessor. Both elections were triggered by the inability of any party to secure an absolute majority, leading to the call for a second 'emergency' election. However, the voter turnout in both of these elections was significantly lower compared to the previous ones, suggesting that the nature of such snap elections may reduce citizens' willingness to vote again.

And apart from the election year and specific election context, how are turnout rates correlated with the municipalities' populations within each election year?

Since the population of municipalities ranges from very small to extremely large, with some major cities having populations hundreds of times greater than small villages, we will apply a logarithmic transformation to the population for a more effective regression analysis.

```{r,warning = FALSE}
cor_value_overall <- cor(question8$population, question8$turnout_rate)
lm_model_overall <- lm(turnout_rate ~ log(population), data = question8)
slope_overall <- coef(lm_model_overall)[2]
p_value_overall <- summary(lm_model_overall)$coefficients[2, 4]

calculate_slope_and_cor <- function(data) {
  data <- data %>%
    mutate(log_population = log(population))
  cor_value <- cor(data$log_population, data$turnout_rate, use = "complete.obs")
  lm_model <- lm(turnout_rate ~ log_population, data = data)
  slope <- coef(lm_model)[2]
  return(data.frame(cor_value = cor_value, slope = slope))
}

slope_cor_values <- question8 %>%
  group_by(date_elec) %>%
  do(calculate_slope_and_cor(.)) %>%
  ungroup()


ggplot(question8, aes(x = log(population), y = turnout_rate)) +
  geom_point(color = "#ffc403") +  
  geom_smooth(method = "lm", color = "#c70b1f", linetype = "dashed") +  
  facet_wrap(~ date_elec) +
  geom_text(data = slope_cor_values, 
            aes(x = max(log(question8$population)), y = max(question8$turnout_rate), 
                label = paste("r = ", round(cor_value, 2), "\nSlope = ", round(slope, 4))), 
            color = "#c70b1f", size = 2, hjust = 1, vjust = 1) +
  theme_minimal() +
  labs(title = "Correlation between log of Population\nand Turnout rate over time",
       subtitle= paste("Combining election results from multiple years, \nthe overall correlation is ", round(cor_value_overall, 2),"\nthe overall slope is ", round(slope_overall, 4), "with a p-value ", p_value_overall, "."))+
   theme(
    plot.title = element_text(face = "bold", hjust = 0, size = 11),
    plot.subtitle = element_text(hjust = 0, size = 9, color = "grey40"),
    axis.title = element_text(size = 10),
    aspect.ratio = 1.2
  )

```

We can observe that the correlation is always negative during this period, the slope is around -0.01 for log(population), and its overall p-value is very close to 0, that is, statistically very significant over time.

The slope -0.01 can be interpreted in this way: If the population doubles, the turnout rate in this municipality will decrease by approximately 0.69%.

**In conclusion, although there is no clear tendency of turnout rate evolution over time, we do observe two important insights:**

1.  The nature of "snap" elections like in 2016 and 2019, as a result of the inability of any party to secure an absolute majority, may reduce citizens' willingness to vote again.
2.  The population size has a negative effect on turnout rate: if the population doubles, the turnout rate in this municipality will decrease by approximately 0.69%.


### 7. Marco, Isabel, Brad

```{r}

```
